{"cells":[{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as pp\n","from sklearn.utils import shuffle\n","import multiprocessing\n","from functools import partial\n","import string\n","from joblib import Parallel, delayed\n","import re\n","import copy\n","import spacy\n","from spacy.lang.en import stop_words as stop_words\n","import nltk\n","from nltk import pos_tag, word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from collections import Counter"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["wnl =WordNetLemmatizer()\n","df = pd.read_csv(\"../ml_insurance_challenge.csv\")\n","num_cores = multiprocessing.cpu_count()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["def replace_word(text):\n","\n","\n","    if len(text) <= 1:\n","        return text\n","    if text[0] == '-':\n","        text = text[1:]\n","    if text[-1] == \"-\":\n","        text = text[:-1]\n","\n","    if \"'s\" not in text and \"'\" in text:\n","        text = text.replace(\"'\",\"\")\n","    return text"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["def replace_word(text):\n","\n","\n","    if len(text) <= 1:\n","        return text\n","    if text[0] == '-' or text[0]==\"'\":\n","        text = text[1:]\n","    if text[-1] == \"-\" or text[-1]==\"'\":\n","        text = text[:-1]\n","\n","    if \"'s\" not in text and \"'\" in text:\n","        text = text.replace(\"'\",\"\")\n","    return text"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["df = df.dropna(axis=0, how='any')\n","colums = [('niche',0), ('description',1), ('category',2), ('business_tags',3), ('sector',4)]\n","category_dict = {0: 'niche', 1: 'description', 2: 'category', 3: 'business_tags', 4: 'sector'}\n","nlp = spacy.load('en_core_web_sm', disable=['tokenizer','parser','ner','textcat','transformer','senter','sentencizer','textcat'])"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def replace_tag(element, tokens):\n","\n","    match tokens:\n","        case 'NUM':\n","            res = any(char.isalpha() for char in element)\n","            if res:\n","                tokens = 'a'\n","                return tokens\n","            return 'n'\n","        case 'ADJ':\n","            tokens = 'a'\n","            return tokens\n","        case 'NOUN':\n","            tokens = 'n'\n","            return tokens\n","        case 'VERB':\n","            tokens = 'v'\n","            return tokens\n","        case 'ADV':\n","            tokens = 'r'\n","            return tokens\n","        case _:\n","            tokens = 'n'\n","            return tokens"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["from nltk.util import ngrams\n"," \n","def new_ngrams(element, n):\n","    for our_grams in ngrams(element, n):\n","            yield  \" \".join(our_grams)\n","\n","def new_ngrams2(element, n):\n","    for our_grams in ngrams(element, n):\n","            return  (\" \".join(our_grams))\n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def lemmatize_f(tokens):\n","\n","    tok = tokens\n","    tokens = nltk.pos_tag(word_tokenize(\" \".join(tokens)), tagset='universal')\n","\n","    tokens = list(map(lambda x:(replace_word(x[0].lower()),replace_tag(x[0],x[1])), tokens))\n","\n","    tokens = list(filter(lambda x: (x[0],x[1]) if x[0].lower() not in stop_words.STOP_WORDS and x[0].lower() not in string.punctuation else '', tokens))\n","\n","\n","    element =  [wnl.lemmatize(token,  idx)+\"_\"+idx for token, idx in tokens]\n","\n","    counter_element = Counter(element)\n","    counter_element.update(new_ngrams(element, 2))\n","    counter_element.update(new_ngrams(element, 3))\n","\n","\n","    return counter_element"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def weighted_frequency(col, new_t,el=None):\n","    if el == None:\n","        el = col\n","    else:\n","        el +=col\n","    match new_t:\n","        case 'niche':\n","            return el+col+col+col\n","        case 'description':\n","            return el+col\n","        case 'category':\n","            return el+col+col+col+col\n","        case 'business_tags':\n","            return el+col+col+col+col+col\n","        case 'sector':\n","            el+=col+col\n","            return el"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["all_punctuation_except_hyphen = string.punctuation.replace(\"-\",\"\").replace(\"'\",\"\").replace(\"â€™\",\"\")\n","df['new_col'] = df['description']\n","\n","df['new_col2'] = df['description']\n","df2 = copy.deepcopy(df)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["for col, idx in colums:\n","    df[col] = df[col].str.replace('[{}]'.format(all_punctuation_except_hyphen), '', regex=True).str.split()\n","    df[col] = [list(filter(lambda x: x if (\"'\" not in x or \"'s\" in x) and (x.isnumeric() == False) else '', el)) for el in df[col]]\n","    df[col] = df[col].apply(lemmatize_f)\n","\n","\n","    if idx == 0:\n","        df['new_col'] = df.apply(lambda x: weighted_frequency(x[col], col),axis=1)\n","    else:\n","        df['new_col'] = df.apply(lambda x: weighted_frequency(x[col], col,x['new_col']),axis=1)\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction import DictVectorizer\n","from scipy.sparse import csr_matrix\n","from sklearn.feature_extraction.text import TfidfTransformer \n","import numpy as np\n","import threading\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","\n","v = DictVectorizer(sparse=True)\n","X = v.fit_transform(df['new_col'])\n","feature_names = v.get_feature_names_out()\n","X_csr = csr_matrix(X)\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n","tf_idf = tfidf_transformer.fit_transform(X_csr)\n"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["num_threads = 8\n","threads = []\n","size = X_csr.shape[0]//num_threads\n","\n","global match_words_with_tf_idf_valuess\n","match_words_with_tf_idf_valuess={}\n","\n","global match_all_words_with_tf_idf_valuess\n","match_all_words_with_tf_idf_valuess={}\n","\n","g=[]\n","global zzz\n","zzz = 0\n","\n","def get_matrix(i, start, end):\n","    for k in range(start, end):\n","        row_tfidf = X[k].toarray()[0]\n","        nonzero_indices = np.nonzero(row_tfidf)[0]\n","        g=0\n","        for j in nonzero_indices:\n","            g+=1\n","            a = round(tf_idf[k,j],5)\n","            \n","            match_words_with_tf_idf_valuess[(feature_names[j], k)] = a\n","    return match_words_with_tf_idf_valuess\n","\n","for i in range(num_threads):\n","    start = i * size\n","    end = (i+1) * size if i != num_threads -1 else X_csr.shape[0]\n","    thread = threading.Thread(target = get_matrix, args=(i, start, end))\n","    threads.append(thread)\n","    thread.start()\n","\n","for thread in threads:\n","    thread.join()\n"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["import nltk\n","from nltk.corpus import wordnet as wn\n","our_classes = pd.read_csv(\"../insurance_taxonomy - insurance_taxonomy.csv\")\n","our_classes_vector = our_classes['label']\n","\n","def our_classes_lemmitize(tokens):\n","   tokens = nltk.pos_tag(word_tokenize(\" \".join(tokens)), tagset='universal')\n","   tokens = list(map(lambda x:(replace_word(x[0].lower()),replace_tag(x[0],x[1])), tokens))\n","   element =  [wnl.lemmatize(token,  idx)+\"_\"+idx for token, idx in tokens]\n","   synonyms = []\n","   #synonyms =  [[list(filter(lambda x: syn.lemmas()[0].name() if x.name() != el[:-2] and (x.name()+\"_\"+el[-1:]) in match_all_words_with_tf_idf_valuess.keys() else '', syn.lemmas())) for syn in wn.synsets(el[:-2], pos=el[-1:])] for el in element]\n","   synonyms=[]\n","   hypernyms = []\n","   hyponyms = []\n","   '''for el in element:\n","      for word_meanings in wn.synsets(el[:-2], pos=el[-1:]):\n","         word_synset =  wn.synset(word_meanings.name())\n","         for syn in word_meanings.lemmas():\n","            word_curated = syn.name()\n","            word_curate_space = word_curated.replace(\"_\", \" \")\n","            \n","            full_syn = syn.name() + \"_\" + el[-1:]\n","            if len(word_curate_space.split(\" \")) > 1:\n","               word_curated = word_curate_space\n","               text_with_tags = nltk.pos_tag(word_tokenize(word_curate_space), tagset='universal')\n","               tokens_ = list(map(lambda x: x[0].lower()+\"_\"+replace_tag(x[0],x[1]), text_with_tags))\n","               full_syn = \" \".join(tokens_)\n","            if syn.name() != el[:-2] and full_syn in match_all_words_with_tf_idf_valuess.keys():\n","               synonyms.append(\"syn_\"+syn.name()+\"_\"+el[-1:])\n","\n","         for hpe in word_meanings.hypernyms():\n","            hpe_synset = wn.synset(hpe.name())\n","            score = word_synset.path_similarity(hpe_synset)\n","            word_curated = hpe.name().split(\".\")[0]\n","            word_curate_space = word_curated.replace(\"_\", \" \")\n","\n","            full_hpe = word_curated + \"_\" + el[-1:]\n","            full_text = \"hpe_\"+full_hpe+\"_\"+str(score)\n","\n","            if len(word_curate_space.split(\" \")) > 1:\n","               #print(word_curate_space)\n","               word_curated = word_curate_space\n","               text_with_tags = nltk.pos_tag(word_tokenize(word_curate_space), tagset='universal')\n","               tokens_ = list(map(lambda x: x[0].lower()+\"_\"+replace_tag(x[0],x[1]), text_with_tags))\n","               full_hpe = \" \".join(tokens_)\n","            \n","            if word_curated != el[:-2] and full_hpe in match_all_words_with_tf_idf_valuess.keys() and full_text not in hypernyms:\n","               hypernyms.append(\"hpe_\"+full_hpe+\"_\"+str(score))\n","\n","         \n","         for hpo in word_meanings.hyponyms():\n","            hpo_synset = wn.synset(hpo.name())\n","            score = word_synset.path_similarity(hpo_synset)\n","            word_curated = hpe.name().split(\".\")[0]\n","            word_curate_space = word_curated.replace(\"_\", \" \")\n","\n","            \n","\n","            full_hpo = word_curated + \"_\" + el[-1:]\n","\n","            if len(word_curate_space.split(\" \")) > 1:\n","               #print(word_curate_space)\n","               word_curated = word_curate_space\n","               text_with_tags = nltk.pos_tag(word_tokenize(word_curate_space), tagset='universal')\n","               tokens_ = list(map(lambda x: x[0].lower()+\"_\"+replace_tag(x[0],x[1]), text_with_tags))\n","               full_hpo = \" \".join(tokens_)\n","\n","            full_text = \"hpo_\"+full_hpo+\"_\"+str(score)\n","\n","\n","            if word_curated != el[:-2] and full_hpo in match_all_words_with_tf_idf_valuess.keys() and full_text not in hyponyms:\n","               hyponyms.append(\"hpo_\"+full_hpo+\"_\"+str(score))'''\n","\n","   #print(hypernyms)\n","   #print(hyponyms)\n","            \n","   element = element +  list(new_ngrams(element,2)) +  list(new_ngrams(element,3))\n","   return element"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["\"listttt=[]\\nfor wdmeanings in wn.synsets('pipeline'):\\n    \\n    for hpe in wdmeanings.hypernyms():\\n       \\n        a = wn.synset(hpe.name())\\n        for hpe2 in a.hypernyms():\\n            listttt.append(hpe2.name())\\n    for hpo in wdmeanings.hyponyms():\\n        b = wn.synset(hpo.name())\\n        for hpo2 in a.hyponyms():\\n            listttt.append(hpo2.name())\\n            #print(hpo2.name())\\n            \\npipeline = wn.synset('liquid.n.01')\\nwater = wn.synset('water.n.02')\\nprint(water.path_similarity(pipeline))\\n\\nprint(listttt)\""]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["'''listttt=[]\n","for wdmeanings in wn.synsets('pipeline'):\n","    \n","    for hpe in wdmeanings.hypernyms():\n","       \n","        a = wn.synset(hpe.name())\n","        for hpe2 in a.hypernyms():\n","            listttt.append(hpe2.name())\n","    for hpo in wdmeanings.hyponyms():\n","        b = wn.synset(hpo.name())\n","        for hpo2 in a.hyponyms():\n","            listttt.append(hpo2.name())\n","            #print(hpo2.name())\n","            \n","pipeline = wn.synset('liquid.n.01')\n","water = wn.synset('water.n.02')\n","print(water.path_similarity(pipeline))\n","\n","print(listttt)'''\n","\n","#print(hpe.name())\n","#print(hpo.name())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["\n","our_classes['label'] = our_classes['label'].replace('[{}]'.format(all_punctuation_except_hyphen), '', regex=True).str.split()\n","our_labels_final = our_classes['label'].apply(our_classes_lemmitize)\n","our_classes['new_label'] = our_labels_final\n","our_classes['rows_number'] = range(0, len(our_labels_final))\n","\n","df['rows_number'] = range(0, len(df['new_col']))\n"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["def match_our_words(word):\n","    word_split = word.split(\"_\")\n","    match word_split[0]: \n","        case \"syn\":\n","            return \"_\".join(word_split[1:])\n","        case \"hpo\" | \"hpe\":\n","            return \"_\".join(word_split[1:-1])\n","        case _:\n","            return word\n","    \n","\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["def multiplier_word(word):\n","    word_split = word.split(\"_\")\n","    match word_split[0]: \n","        case \"hpo\" | \"hpe\":\n","            return float(word_split[-1])\n","        case _:\n","            return 1.0\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","embeddings_index = {}\n","\n","f = open('../numberbatch-en.txt')\n","\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    num = word.split(\" \")\n","    if len(num) > 1:\n","        print(word)\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Counter({'connection_n': 20, 'network_n': 12, 'engineering_n': 11, 'network_n connection_n': 10, 'civil_n': 9, 'civil_n engineering_n': 9, 'design_n': 8, 'service_n': 8, 'construction_n': 6, 'engineering_n construction_n': 6, 'electricity_n': 6, 'gas_n': 6, 'project_n': 6, 'optic_n': 6, 'connection_n design_n': 6, 'design_n connection_n': 6, 'connection_n optic_n': 6, 'network_n connection_n design_n': 6, 'connection_n design_n connection_n': 6, 'design_n connection_n optic_n': 6, 'engineering_n service_n': 5, 'civil_n engineering_n service_n': 5, 'heavy_n': 4, 'heavy_n civil_n': 4, 'heavy_n civil_n engineering_n': 4, 'civil_n engineering_n construction_n': 4, 'welchcivils_n': 4, 'design_v': 4, 'solution_n': 4, 'water_n': 4, 'exist_v': 4, 'civil_a': 2, 'company_n': 2, 'specialize_v': 2, 'building_n': 2, 'utility_n': 2, 'uk_n': 2, 'offer_v': 2, 'multi-utility_a': 2, 'combine_v': 2, 'fibre_a': 2, 'optic_a': 2, 'installation_n': 2, 'single_a': 2, 'contract_n': 2, 'engineer_n': 2, 'team_n': 2, 'capable_a': 2, 'point_n': 2, 'meter_v': 2, 'location_n': 2, 'development_n': 2, 'management_n': 2, 'reinforcement_n': 2, 'diversion_n': 2, 'provide_v': 2, 'custom_a': 2, 'account_n': 2, 'asset_n': 2, 'maximize_v': 2, 'usage_n': 2, 'trench_n': 2, 'meet_n': 2, 'deadline_n': 2, 'considerable_a': 2, 'expertise_n': 2, 'instal_v': 2, 'variety_n': 2, 'market_n': 2, 'category_n': 2, 'include_v': 2, 'residential_a': 2, 'commercial_a': 2, 'industrial_a': 2, 'welchcivils_n civil_a': 2, 'civil_a engineering_n': 2, 'construction_n company_n': 2, 'company_n specialize_v': 2, 'specialize_v design_v': 2, 'design_v building_n': 2, 'building_n utility_n': 2, 'utility_n network_n': 2, 'connection_n uk_n': 2, 'uk_n offer_v': 2, 'offer_v multi-utility_a': 2, 'multi-utility_a solution_n': 2, 'solution_n combine_v': 2, 'combine_v electricity_n': 2, 'electricity_n gas_n': 2, 'gas_n water_n': 2, 'water_n fibre_a': 2, 'fibre_a optic_a': 2, 'optic_a installation_n': 2, 'installation_n single_a': 2, 'single_a contract_n': 2, 'contract_n design_n': 2, 'design_n engineer_n': 2, 'engineer_n team_n': 2, 'team_n capable_a': 2, 'capable_a design_v': 2, 'design_v electricity_n': 2, 'electricity_n water_n': 2, 'water_n gas_n': 2, 'gas_n network_n': 2, 'network_n exist_v': 2, 'exist_v network_n': 2, 'connection_n point_n': 2, 'point_n meter_v': 2, 'meter_v location_n': 2, 'location_n development_n': 2, 'development_n project_n': 2, 'project_n management_n': 2, 'management_n reinforcement_n': 2, 'reinforcement_n diversion_n': 2, 'diversion_n provide_v': 2, 'provide_v custom_a': 2, 'custom_a connection_n': 2, 'connection_n solution_n': 2, 'solution_n account_n': 2, 'account_n exist_v': 2, 'exist_v asset_n': 2, 'asset_n maximize_v': 2, 'maximize_v usage_n': 2, 'usage_n trench_n': 2, 'trench_n meet_n': 2, 'meet_n project_n': 2, 'project_n deadline_n': 2, 'deadline_n welchcivils_n': 2, 'welchcivils_n considerable_a': 2, 'considerable_a expertise_n': 2, 'expertise_n instal_v': 2, 'instal_v gas_n': 2, 'gas_n electricity_n': 2, 'electricity_n connection_n': 2, 'connection_n variety_n': 2, 'variety_n market_n': 2, 'market_n category_n': 2, 'category_n include_v': 2, 'include_v residential_a': 2, 'residential_a commercial_a': 2, 'commercial_a industrial_a': 2, 'industrial_a project_n': 2, 'welchcivils_n civil_a engineering_n': 2, 'civil_a engineering_n construction_n': 2, 'engineering_n construction_n company_n': 2, 'construction_n company_n specialize_v': 2, 'company_n specialize_v design_v': 2, 'specialize_v design_v building_n': 2, 'design_v building_n utility_n': 2, 'building_n utility_n network_n': 2, 'utility_n network_n connection_n': 2, 'network_n connection_n uk_n': 2, 'connection_n uk_n offer_v': 2, 'uk_n offer_v multi-utility_a': 2, 'offer_v multi-utility_a solution_n': 2, 'multi-utility_a solution_n combine_v': 2, 'solution_n combine_v electricity_n': 2, 'combine_v electricity_n gas_n': 2, 'electricity_n gas_n water_n': 2, 'gas_n water_n fibre_a': 2, 'water_n fibre_a optic_a': 2, 'fibre_a optic_a installation_n': 2, 'optic_a installation_n single_a': 2, 'installation_n single_a contract_n': 2, 'single_a contract_n design_n': 2, 'contract_n design_n engineer_n': 2, 'design_n engineer_n team_n': 2, 'engineer_n team_n capable_a': 2, 'team_n capable_a design_v': 2, 'capable_a design_v electricity_n': 2, 'design_v electricity_n water_n': 2, 'electricity_n water_n gas_n': 2, 'water_n gas_n network_n': 2, 'gas_n network_n exist_v': 2, 'network_n exist_v network_n': 2, 'exist_v network_n connection_n': 2, 'network_n connection_n point_n': 2, 'connection_n point_n meter_v': 2, 'point_n meter_v location_n': 2, 'meter_v location_n development_n': 2, 'location_n development_n project_n': 2, 'development_n project_n management_n': 2, 'project_n management_n reinforcement_n': 2, 'management_n reinforcement_n diversion_n': 2, 'reinforcement_n diversion_n provide_v': 2, 'diversion_n provide_v custom_a': 2, 'provide_v custom_a connection_n': 2, 'custom_a connection_n solution_n': 2, 'connection_n solution_n account_n': 2, 'solution_n account_n exist_v': 2, 'account_n exist_v asset_n': 2, 'exist_v asset_n maximize_v': 2, 'asset_n maximize_v usage_n': 2, 'maximize_v usage_n trench_n': 2, 'usage_n trench_n meet_n': 2, 'trench_n meet_n project_n': 2, 'meet_n project_n deadline_n': 2, 'project_n deadline_n welchcivils_n': 2, 'deadline_n welchcivils_n considerable_a': 2, 'welchcivils_n considerable_a expertise_n': 2, 'considerable_a expertise_n instal_v': 2, 'expertise_n instal_v gas_n': 2, 'instal_v gas_n electricity_n': 2, 'gas_n electricity_n connection_n': 2, 'electricity_n connection_n variety_n': 2, 'connection_n variety_n market_n': 2, 'variety_n market_n category_n': 2, 'market_n category_n include_v': 2, 'category_n include_v residential_a': 2, 'include_v residential_a commercial_a': 2, 'residential_a commercial_a industrial_a': 2, 'commercial_a industrial_a project_n': 2})\n"]}],"source":["from functools import reduce\n","def embedding_sentence1(sentence1):\n","    sen1 = sentence1\n","    if (len(sentence1.split(\"_\")) > 1):\n","        t = sen1.split(\"_\")\n","        sen1 = \"\"\n","        sen1 +=t[0]\n","        sen1 += \"\".join(reduce(lambda x,y: x+\"_\"+y[2:], t[1:], \"\"))\n","\n","    vec1 = embeddings_index.get(sen1, np.zeros(300))\n","    return vec1\n","\n","\n","\n","def get_similarity_score(vec1, vec2):\n","    return cosine_similarity(vec1, vec2)[0][0]\n","print(df['new_col'][0])\n"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(9455, 220)\n"]}],"source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer\n","\n","bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n","\n","our_classes_vector_embeddings = bert_model.encode(our_classes_vector)\n","\n","texts_vector_description = []\n","company_texts_description = [\n","    f\"{row['description']}\"\n","    for index, row in df2.iterrows()\n","]\n","company_texts_business_tags = [\n","    f\"{' '.join(row['business_tags'])}\"\n","    for index, row in df2.iterrows()\n","]\n","company_texts_sector = [\n","    f\"{row['sector']}\"\n","    for index, row in df2.iterrows()\n","]\n","company_texts_category = [\n","    f\"{row['category']}\"\n","    for index, row in df2.iterrows()\n","]\n","company_texts_niche = [\n","    f\"{row['niche']}\"\n","    for index, row in df2.iterrows()\n","]\n","\n","\n","\n","company_embeddings_1 = bert_model.encode(company_texts_description, batch_size=32) \n","company_embeddings_2 = bert_model.encode(company_texts_business_tags, batch_size=32)  \n","company_embeddings_3 = bert_model.encode(company_texts_sector, batch_size=32) \n","company_embeddings_4 = bert_model.encode(company_texts_category, batch_size=32) \n","company_embeddings_5 = bert_model.encode(company_texts_niche, batch_size=32) \n","\n","similarity_matrix = cosine_similarity(company_embeddings_1, our_classes_vector_embeddings)\n","similarity_matrix += cosine_similarity(company_embeddings_2, our_classes_vector_embeddings)\n","similarity_matrix += cosine_similarity(company_embeddings_3, our_classes_vector_embeddings)\n","similarity_matrix += cosine_similarity(company_embeddings_4, our_classes_vector_embeddings)\n","similarity_matrix += cosine_similarity(company_embeddings_5, our_classes_vector_embeddings)\n","\n","similarity_matrix /=5\n","print(similarity_matrix.shape)\n","\n"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["'''best_matches = [np.argpartition(similarity_matrix[i], -30)[-30:] for i in range(0, len(df2))]'''"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"data":{"text/plain":["\"def get_best_match(word_label, var2):\\n\\n    for index in best_matches[var2]:\\n        print(index)\\n\\n        #for word_label in our_labels_final[index]:\\n            #print(i)\\n            #ff\\n        #sum11 = 0.0\\n            #print(list(df['new_col'].iloc(i)))\\n            #sum2+=reduce(lambda x, y: x + get_similarity_score(y[:-2], word_label[:-2]) * (match_words_with_tf_idf_valuess[(y, i)]) if get_similarity_score(y[:-2], word_label[:-2]) >=0.2 else x, df['new_col'][i], 0.0)\\n\\n    return 0\""]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["'''def get_best_match(word_label, var2):\n","\n","    for index in best_matches[var2]:\n","        print(index)\n","\n","        #for word_label in our_labels_final[index]:\n","            #print(i)\n","            #ff\n","        #sum11 = 0.0\n","            #print(list(df['new_col'].iloc(i)))\n","            #sum2+=reduce(lambda x, y: x + get_similarity_score(y[:-2], word_label[:-2]) * (match_words_with_tf_idf_valuess[(y, i)]) if get_similarity_score(y[:-2], word_label[:-2]) >=0.2 else x, df['new_col'][i], 0.0)\n","\n","    return 0'''"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["from functools import reduce\n","\n","vec_embeddings=[]\n","label_embeddings = []\n","MAX_TOKENS = max(df['new_col'].apply(len))\n","\n","global rows_\n","\n","def get_sentence_embeddings(word_list, row, opt):\n","    embeddings_ = []\n","    if opt == 1:\n","\n","        sum_list_tf_idf = sum(match_words_with_tf_idf_valuess[(\n","            word,\n","            row \n","        )] for word in word_list)\n","        embeddings_ = [\n","        (match_words_with_tf_idf_valuess[(\n","            word,\n","            row \n","        )] * embeddings_index.get(\n","            word.split(\"_\")[0] + \"\".join(\n","                reduce(\n","                    lambda x, y: x + \"_\" + y[2:] if len(word.split(\"_\")) > 2 else \"\", \n","                    word[:-2].split(\"_\")[1:], \n","                    \"\"\n","                )\n","            ),\n","            np.zeros(300)\n","        ))/ sum_list_tf_idf\n","            for word in word_list\n","        ]\n","    else:\n","        embeddings_ = [\n","        embeddings_index.get(\n","            word.split(\"_\")[0] + \"\".join(\n","                reduce(\n","                    lambda x, y: x + \"_\" + y[2:] if len(word.split(\"_\")) > 2 else \"\", \n","                    word[:-2].split(\"_\")[1:], \n","                    \"\"\n","                )\n","            ),\n","            np.zeros(300)\n","        )\n","            for word in word_list\n","        ]\n","\n","\n","    embeddings_ = np.mean(embeddings_, axis=0)\n","    if (len(embeddings_)) < X.shape[1]:\n","        embeddings_= np.pad(embeddings_, (0, (400-len(embeddings_))), mode = 'constant')\n","\n","    return embeddings_\n","\n","vec_embeddings = np.array(df.apply(lambda x: get_sentence_embeddings(x['new_col'], x['rows_number'], 1), axis=1).to_list())\n","\n","label_embeddings = np.array(our_classes.apply(lambda x: get_sentence_embeddings(x['new_label'], x['rows_number'], 2), axis=1).to_list())\n","del(match_words_with_tf_idf_valuess)\n","del(match_all_words_with_tf_idf_valuess)\n","\n"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["np.savetxt(\"vec_embeddings.txt\", vec_embeddings, fmt=\"%.12f\")\n","np.savetxt(\"label_embeddings.txt\", label_embeddings, fmt=\"%.12f\")\n","np.savetxt(\"context_matrix.txt\", similarity_matrix, fmt=\"%.12f\")"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":2}
